# Gemini FastAPI Demo üöÄ

A simple **FastAPI + LangChain + Google Gemini** web application that demonstrates how to build an AI-powered Q&A interface using a clean UI.

This project is meant for **learning, experimentation, and interview-ready demos**.

---

## ‚ú® Features

* üåê FastAPI backend
* ü§ñ Google Gemini (via LangChain)
* üß† Uses `gemini-2.5-flash` model
* üé® HTML UI with Jinja2 templates
* üìÑ Form-based question input
* ‚ö° Quick setup and lightweight

---

## üõ† Tech Stack

* **Python**
* **FastAPI**
* **LangChain**
* **Google Generative AI (Gemini)**
* **Jinja2 Templates**
* **HTML / CSS**

---

## üìÅ Project Structure

```
.
‚îú‚îÄ‚îÄ main.py              # FastAPI application
‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îî‚îÄ‚îÄ index.html       # UI template
‚îú‚îÄ‚îÄ static/              # Static files (CSS, JS)
‚îú‚îÄ‚îÄ .env                 # Environment variables
‚îú‚îÄ‚îÄ requirements.txt     # Python dependencies
‚îî‚îÄ‚îÄ README.md
```

---

## üîê Environment Setup

Create a `.env` file in the root directory and add your **Google Gemini API key**:

```
GOOGLE_API_KEY=your_api_key_here
```

> ‚ö†Ô∏è Do NOT commit your `.env` file to GitHub.

---

## ‚ñ∂Ô∏è How to Run Locally

### 1Ô∏è‚É£ Clone the repository

```bash
git clone https://github.com/karan9970/fastapi-learning-log
cd https://github.com/karan9970/fastapi-learning-log
```

### 2Ô∏è‚É£ Create a virtual environment (optional but recommended)

```bash
python -m venv venv
source venv/bin/activate   # Linux/Mac
venv\Scripts\activate      # Windows
```

### 3Ô∏è‚É£ Install dependencies

```bash
pip install -r requirements.txt
```

### 4Ô∏è‚É£ Start the FastAPI server

```bash
uvicorn main:app --reload
```

### 5Ô∏è‚É£ Open in browser

```
http://127.0.0.1:8000
```

---

## üß† How It Works

1. User enters a question in the web UI
2. FastAPI receives the form submission
3. LangChain sends the prompt to **Gemini**
4. Gemini generates a response
5. The answer is rendered back in the UI

---

## üìå Example Code Snippet

```python
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    temperature=1.0
)

result = llm.invoke(question)
answer = result.content
```

---

## üöß Future Improvements

* Add streaming responses
* Chat history support
* Better UI styling
* Error handling & validation
* Model selection toggle

---

## üéØ Use Case

* AI learning projects
* LangChain + Gemini demos
* FastAPI interview showcase

This project is open-source and free to use for learning and experimentation.

### ‚≠ê If you like this project, give it a star!
